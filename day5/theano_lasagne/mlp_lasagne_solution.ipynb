{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plain vanilla Lasagne \n",
    "This notebook demonstrates who to use plain vanila lasgane, without the nolearn helper functions, for a multilayer perceptorn\n",
    "\n",
    "\n",
    "# Minimal Lasagne Demo (multilayer perceptron)\n",
    "In this script we build a small multilayer perceptron with two hidden layers having 500 and 50 neurons each for classifying the MNIST database of handwritten digits. It uses plain vanilla lasagne, it's meant for an introduction and thus does not uses fancy stuff such as dropout layers. The code has been taken from: https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\n",
    "\n",
    "\n",
    "## Loading the data\n",
    "While the original dataset has 70'000 examples of hand written digits (see below for loading) we restrict us here to the first 4000 digits so that an interactive session is possible. The subset of the first 4000 MNIST examples is provided in the repository for convenience. \n",
    "\n",
    "### Data-Format\n",
    "For Lasagne all images must be provided in a 4-dimensional array X with the following dimensions (number of images, number of colors, x, y) and the labels in a vector y of same size. So X[1,0,1,2] would be the image number 1, the color channel 0 and the pixel x = 1 and y=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 1, 28, 28), (4000,), 28)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imgplot\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import cPickle as pickle\n",
    "import gzip\n",
    "import time\n",
    "\n",
    "with gzip.open('../mnist_4000.pkl.gz', 'rb') as f:\n",
    "    (X,y) = pickle.load(f)\n",
    "PIXELS = len(X[0,0,0,:])\n",
    "X.shape, y.shape, PIXELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0584806"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,0,15,16] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of the data\n",
    "The data is normalized so that each pixel has mean 0 and standard deviation close to 1 (calculated over all images). This has been done with the following code (the 0th axis is along the images): \n",
    "```\n",
    "    Xmean = X.mean(axis = 0)\n",
    "    XStd = np.sqrt(X.var(axis=0))\n",
    "    X = (X-Xmean)/(XStd + 0.01)\n",
    "```\n",
    "Note that with this normalization the mean values of all pixels of a single image need not be 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.046298616, 0.0, 0.0, 0.93183041)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X[0,0,:,:]),np.mean(X[:,0,1,1]),np.mean(X[:,0,2,1]),np.var(X[:,0,10,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let s plot the first few images and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAADCCAYAAABZnYY7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAAAAAEL9P8kFo0umBYwBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10efcf750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10,30))\n",
    "for i in range(3):\n",
    "    a=fig.add_subplot(1,3,(i+1))\n",
    "    plt.imshow(-X[i,0,:,:], interpolation='none',cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the backgound is not uniform. That is due to the batchnormalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first simple network\n",
    "Let's define a MLP. We start with the architecture. All we do until the compling is using symbolic functions.\n",
    "\n",
    "#### Definition of the network (architecture)\n",
    "\n",
    "* An Input Layer with the following 4-dimensions: \n",
    "    * 0: Batch Size yet unkown hence `None`\n",
    "    * 1: one \"color\" channel\n",
    "    * 2,3: (28,28) pixels\n",
    "* A hidden layer with 500 units\n",
    "* A second hidden layer with 50 units\n",
    "* An output layer with 10 units\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n"
     ]
    }
   ],
   "source": [
    "input_var = T.tensor4('inputs') #This is a variable needed \n",
    "# 1st Layer\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, 1, 28, 28), input_var=input_var) #None depend on batch size\n",
    "# 2nd Layer\n",
    "l_hid1 = lasagne.layers.DenseLayer(l_in, num_units=500, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "# 3rd Layer\n",
    "l_hid2 = lasagne.layers.DenseLayer(l_hid1, num_units=50, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "# Last Layer\n",
    "network = lasagne.layers.DenseLayer(l_hid2, num_units=10, nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "To optimize the network we use the log-loss of the last layer. The log loss in lasagne is called 'lasagne.objectives.categorical_crossentropy'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/lasagne/layers/helper.py:69: UserWarning: get_all_layers() has been changed to return layers in topological order. The former implementation is still available as get_all_layers_old(), but will be removed before the first release of Lasagne. To ignore this warning, use `warnings.filterwarnings('ignore', '.*topo.*')`.\n",
      "  warnings.warn(\"get_all_layers() has been changed to return layers in \"\n"
     ]
    }
   ],
   "source": [
    "target_var = T.ivector('targets') #The classes 0..9\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlling the learning\n",
    "Create update expressions for training, i.e., how to modify the parameters at each training step. Here, we'll use Stochastic Gradient Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the function for training\n",
    "We know have everything in place to compile a function describing the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compling a function for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var), dtype=theano.config.floatX)\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "After the definition of the network the network has to be trained. We use the first 2400 samples for training and the 600 images from 2400-3000 for validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[0:2400,:,:,:]\n",
    "X_val   = X[2400:3000,:,:,:]\n",
    "y_train = y[0:2400]\n",
    "y_val = y[2400:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################## Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "perf = pd.DataFrame(columns=['train_loss','valid_loss','valid_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 0.163s\n",
      "  training loss:\t\t2.285374\n",
      "  validation loss:\t\t1.091113\n",
      "  validation accuracy:\t\t21.00 %\n",
      "Epoch 2 of 100 took 0.152s\n",
      "  training loss:\t\t2.100532\n",
      "  validation loss:\t\t0.996479\n",
      "  validation accuracy:\t\t62.83 %\n",
      "Epoch 3 of 100 took 0.147s\n",
      "  training loss:\t\t1.909607\n",
      "  validation loss:\t\t0.894727\n",
      "  validation accuracy:\t\t69.67 %\n",
      "Epoch 4 of 100 took 0.144s\n",
      "  training loss:\t\t1.702203\n",
      "  validation loss:\t\t0.787833\n",
      "  validation accuracy:\t\t73.33 %\n",
      "Epoch 5 of 100 took 0.144s\n",
      "  training loss:\t\t1.495270\n",
      "  validation loss:\t\t0.688070\n",
      "  validation accuracy:\t\t73.67 %\n",
      "Epoch 6 of 100 took 0.142s\n",
      "  training loss:\t\t1.304386\n",
      "  validation loss:\t\t0.601403\n",
      "  validation accuracy:\t\t76.50 %\n",
      "Epoch 7 of 100 took 0.144s\n",
      "  training loss:\t\t1.140071\n",
      "  validation loss:\t\t0.527516\n",
      "  validation accuracy:\t\t78.17 %\n",
      "Epoch 8 of 100 took 0.139s\n",
      "  training loss:\t\t1.003855\n",
      "  validation loss:\t\t0.469065\n",
      "  validation accuracy:\t\t81.83 %\n",
      "Epoch 9 of 100 took 0.142s\n",
      "  training loss:\t\t0.893384\n",
      "  validation loss:\t\t0.422522\n",
      "  validation accuracy:\t\t83.33 %\n",
      "Epoch 10 of 100 took 0.158s\n",
      "  training loss:\t\t0.805140\n",
      "  validation loss:\t\t0.385958\n",
      "  validation accuracy:\t\t83.50 %\n",
      "Epoch 11 of 100 took 0.141s\n",
      "  training loss:\t\t0.732107\n",
      "  validation loss:\t\t0.354443\n",
      "  validation accuracy:\t\t84.17 %\n",
      "Epoch 12 of 100 took 0.153s\n",
      "  training loss:\t\t0.673126\n",
      "  validation loss:\t\t0.330976\n",
      "  validation accuracy:\t\t83.83 %\n",
      "Epoch 13 of 100 took 0.140s\n",
      "  training loss:\t\t0.623157\n",
      "  validation loss:\t\t0.311107\n",
      "  validation accuracy:\t\t84.83 %\n",
      "Epoch 14 of 100 took 0.142s\n",
      "  training loss:\t\t0.581427\n",
      "  validation loss:\t\t0.295079\n",
      "  validation accuracy:\t\t85.17 %\n",
      "Epoch 15 of 100 took 0.141s\n",
      "  training loss:\t\t0.545329\n",
      "  validation loss:\t\t0.278929\n",
      "  validation accuracy:\t\t86.17 %\n",
      "Epoch 16 of 100 took 0.140s\n",
      "  training loss:\t\t0.514590\n",
      "  validation loss:\t\t0.269195\n",
      "  validation accuracy:\t\t85.17 %\n",
      "Epoch 17 of 100 took 0.140s\n",
      "  training loss:\t\t0.486901\n",
      "  validation loss:\t\t0.257007\n",
      "  validation accuracy:\t\t86.83 %\n",
      "Epoch 18 of 100 took 0.141s\n",
      "  training loss:\t\t0.463256\n",
      "  validation loss:\t\t0.246955\n",
      "  validation accuracy:\t\t87.00 %\n",
      "Epoch 19 of 100 took 0.145s\n",
      "  training loss:\t\t0.442218\n",
      "  validation loss:\t\t0.239686\n",
      "  validation accuracy:\t\t87.67 %\n",
      "Epoch 20 of 100 took 0.141s\n",
      "  training loss:\t\t0.423653\n",
      "  validation loss:\t\t0.232710\n",
      "  validation accuracy:\t\t87.83 %\n",
      "Epoch 21 of 100 took 0.142s\n",
      "  training loss:\t\t0.407129\n",
      "  validation loss:\t\t0.227932\n",
      "  validation accuracy:\t\t87.67 %\n",
      "Epoch 22 of 100 took 0.141s\n",
      "  training loss:\t\t0.390284\n",
      "  validation loss:\t\t0.221375\n",
      "  validation accuracy:\t\t88.33 %\n",
      "Epoch 23 of 100 took 0.144s\n",
      "  training loss:\t\t0.375806\n",
      "  validation loss:\t\t0.216312\n",
      "  validation accuracy:\t\t88.33 %\n",
      "Epoch 24 of 100 took 0.158s\n",
      "  training loss:\t\t0.363372\n",
      "  validation loss:\t\t0.213110\n",
      "  validation accuracy:\t\t88.33 %\n",
      "Epoch 25 of 100 took 0.143s\n",
      "  training loss:\t\t0.351683\n",
      "  validation loss:\t\t0.207724\n",
      "  validation accuracy:\t\t88.67 %\n",
      "Epoch 26 of 100 took 0.143s\n",
      "  training loss:\t\t0.340696\n",
      "  validation loss:\t\t0.204076\n",
      "  validation accuracy:\t\t88.50 %\n",
      "Epoch 27 of 100 took 0.152s\n",
      "  training loss:\t\t0.330611\n",
      "  validation loss:\t\t0.199788\n",
      "  validation accuracy:\t\t89.00 %\n",
      "Epoch 28 of 100 took 0.150s\n",
      "  training loss:\t\t0.321272\n",
      "  validation loss:\t\t0.198498\n",
      "  validation accuracy:\t\t88.83 %\n",
      "Epoch 29 of 100 took 0.143s\n",
      "  training loss:\t\t0.311855\n",
      "  validation loss:\t\t0.194816\n",
      "  validation accuracy:\t\t89.17 %\n",
      "Epoch 30 of 100 took 0.157s\n",
      "  training loss:\t\t0.303576\n",
      "  validation loss:\t\t0.193171\n",
      "  validation accuracy:\t\t89.33 %\n",
      "Epoch 31 of 100 took 0.185s\n",
      "  training loss:\t\t0.295467\n",
      "  validation loss:\t\t0.189775\n",
      "  validation accuracy:\t\t89.33 %\n",
      "Epoch 32 of 100 took 0.157s\n",
      "  training loss:\t\t0.287239\n",
      "  validation loss:\t\t0.189045\n",
      "  validation accuracy:\t\t89.33 %\n",
      "Epoch 33 of 100 took 0.173s\n",
      "  training loss:\t\t0.281477\n",
      "  validation loss:\t\t0.186517\n",
      "  validation accuracy:\t\t89.33 %\n",
      "Epoch 34 of 100 took 0.157s\n",
      "  training loss:\t\t0.274610\n",
      "  validation loss:\t\t0.184397\n",
      "  validation accuracy:\t\t89.67 %\n",
      "Epoch 35 of 100 took 0.143s\n",
      "  training loss:\t\t0.267700\n",
      "  validation loss:\t\t0.182644\n",
      "  validation accuracy:\t\t89.67 %\n",
      "Epoch 36 of 100 took 0.142s\n",
      "  training loss:\t\t0.261380\n",
      "  validation loss:\t\t0.181642\n",
      "  validation accuracy:\t\t89.83 %\n",
      "Epoch 37 of 100 took 0.144s\n",
      "  training loss:\t\t0.255566\n",
      "  validation loss:\t\t0.179415\n",
      "  validation accuracy:\t\t89.83 %\n",
      "Epoch 38 of 100 took 0.138s\n",
      "  training loss:\t\t0.249643\n",
      "  validation loss:\t\t0.178458\n",
      "  validation accuracy:\t\t89.83 %\n",
      "Epoch 39 of 100 took 0.160s\n",
      "  training loss:\t\t0.245081\n",
      "  validation loss:\t\t0.177803\n",
      "  validation accuracy:\t\t90.00 %\n",
      "Epoch 40 of 100 took 0.154s\n",
      "  training loss:\t\t0.239730\n",
      "  validation loss:\t\t0.176023\n",
      "  validation accuracy:\t\t90.17 %\n",
      "Epoch 41 of 100 took 0.139s\n",
      "  training loss:\t\t0.234726\n",
      "  validation loss:\t\t0.174354\n",
      "  validation accuracy:\t\t90.17 %\n",
      "Epoch 42 of 100 took 0.139s\n",
      "  training loss:\t\t0.230798\n",
      "  validation loss:\t\t0.174905\n",
      "  validation accuracy:\t\t90.17 %\n",
      "Epoch 43 of 100 took 0.136s\n",
      "  training loss:\t\t0.225612\n",
      "  validation loss:\t\t0.172789\n",
      "  validation accuracy:\t\t90.33 %\n",
      "Epoch 44 of 100 took 0.150s\n",
      "  training loss:\t\t0.220726\n",
      "  validation loss:\t\t0.172189\n",
      "  validation accuracy:\t\t90.33 %\n",
      "Epoch 45 of 100 took 0.151s\n",
      "  training loss:\t\t0.216885\n",
      "  validation loss:\t\t0.171472\n",
      "  validation accuracy:\t\t90.33 %\n",
      "Epoch 46 of 100 took 0.152s\n",
      "  training loss:\t\t0.213091\n",
      "  validation loss:\t\t0.170846\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 47 of 100 took 0.153s\n",
      "  training loss:\t\t0.209737\n",
      "  validation loss:\t\t0.169972\n",
      "  validation accuracy:\t\t90.67 %\n",
      "Epoch 48 of 100 took 0.138s\n",
      "  training loss:\t\t0.204803\n",
      "  validation loss:\t\t0.168697\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 49 of 100 took 0.146s\n",
      "  training loss:\t\t0.201860\n",
      "  validation loss:\t\t0.168238\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 50 of 100 took 0.135s\n",
      "  training loss:\t\t0.197603\n",
      "  validation loss:\t\t0.167298\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 51 of 100 took 0.135s\n",
      "  training loss:\t\t0.193622\n",
      "  validation loss:\t\t0.167556\n",
      "  validation accuracy:\t\t90.50 %\n",
      "Epoch 52 of 100 took 0.134s\n",
      "  training loss:\t\t0.190542\n",
      "  validation loss:\t\t0.166277\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 53 of 100 took 0.140s\n",
      "  training loss:\t\t0.187135\n",
      "  validation loss:\t\t0.165565\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 54 of 100 took 0.137s\n",
      "  training loss:\t\t0.183923\n",
      "  validation loss:\t\t0.165499\n",
      "  validation accuracy:\t\t90.67 %\n",
      "Epoch 55 of 100 took 0.136s\n",
      "  training loss:\t\t0.181392\n",
      "  validation loss:\t\t0.165298\n",
      "  validation accuracy:\t\t91.17 %\n",
      "Epoch 56 of 100 took 0.135s\n",
      "  training loss:\t\t0.177382\n",
      "  validation loss:\t\t0.164202\n",
      "  validation accuracy:\t\t91.17 %\n",
      "Epoch 57 of 100 took 0.135s\n",
      "  training loss:\t\t0.174773\n",
      "  validation loss:\t\t0.164051\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 58 of 100 took 0.134s\n",
      "  training loss:\t\t0.171907\n",
      "  validation loss:\t\t0.163157\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 59 of 100 took 0.135s\n",
      "  training loss:\t\t0.168878\n",
      "  validation loss:\t\t0.163538\n",
      "  validation accuracy:\t\t91.17 %\n",
      "Epoch 60 of 100 took 0.135s\n",
      "  training loss:\t\t0.166877\n",
      "  validation loss:\t\t0.163241\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 61 of 100 took 0.135s\n",
      "  training loss:\t\t0.163368\n",
      "  validation loss:\t\t0.163185\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 62 of 100 took 0.135s\n",
      "  training loss:\t\t0.160819\n",
      "  validation loss:\t\t0.162799\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 63 of 100 took 0.136s\n",
      "  training loss:\t\t0.158569\n",
      "  validation loss:\t\t0.161995\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 64 of 100 took 0.135s\n",
      "  training loss:\t\t0.155992\n",
      "  validation loss:\t\t0.162255\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 65 of 100 took 0.135s\n",
      "  training loss:\t\t0.153501\n",
      "  validation loss:\t\t0.161879\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 66 of 100 took 0.137s\n",
      "  training loss:\t\t0.151430\n",
      "  validation loss:\t\t0.161632\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 67 of 100 took 0.135s\n",
      "  training loss:\t\t0.148783\n",
      "  validation loss:\t\t0.161593\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 68 of 100 took 0.134s\n",
      "  training loss:\t\t0.146798\n",
      "  validation loss:\t\t0.160746\n",
      "  validation accuracy:\t\t91.17 %\n",
      "Epoch 69 of 100 took 0.135s\n",
      "  training loss:\t\t0.144323\n",
      "  validation loss:\t\t0.160975\n",
      "  validation accuracy:\t\t91.17 %\n",
      "Epoch 70 of 100 took 0.136s\n",
      "  training loss:\t\t0.142287\n",
      "  validation loss:\t\t0.160725\n",
      "  validation accuracy:\t\t91.17 %\n",
      "Epoch 71 of 100 took 0.135s\n",
      "  training loss:\t\t0.140078\n",
      "  validation loss:\t\t0.160862\n",
      "  validation accuracy:\t\t91.17 %\n",
      "Epoch 72 of 100 took 0.137s\n",
      "  training loss:\t\t0.138752\n",
      "  validation loss:\t\t0.160211\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 73 of 100 took 0.136s\n",
      "  training loss:\t\t0.136581\n",
      "  validation loss:\t\t0.160501\n",
      "  validation accuracy:\t\t91.17 %\n",
      "Epoch 74 of 100 took 0.136s\n",
      "  training loss:\t\t0.134042\n",
      "  validation loss:\t\t0.159881\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 75 of 100 took 0.137s\n",
      "  training loss:\t\t0.131988\n",
      "  validation loss:\t\t0.160466\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 76 of 100 took 0.137s\n",
      "  training loss:\t\t0.130029\n",
      "  validation loss:\t\t0.160093\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 77 of 100 took 0.138s\n",
      "  training loss:\t\t0.128509\n",
      "  validation loss:\t\t0.159979\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 78 of 100 took 0.135s\n",
      "  training loss:\t\t0.126629\n",
      "  validation loss:\t\t0.160384\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 79 of 100 took 0.135s\n",
      "  training loss:\t\t0.125116\n",
      "  validation loss:\t\t0.160029\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 80 of 100 took 0.135s\n",
      "  training loss:\t\t0.122795\n",
      "  validation loss:\t\t0.159749\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 81 of 100 took 0.135s\n",
      "  training loss:\t\t0.121261\n",
      "  validation loss:\t\t0.159361\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 82 of 100 took 0.135s\n",
      "  training loss:\t\t0.119488\n",
      "  validation loss:\t\t0.160101\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 83 of 100 took 0.135s\n",
      "  training loss:\t\t0.117998\n",
      "  validation loss:\t\t0.159457\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 84 of 100 took 0.135s\n",
      "  training loss:\t\t0.116252\n",
      "  validation loss:\t\t0.159784\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 85 of 100 took 0.137s\n",
      "  training loss:\t\t0.114413\n",
      "  validation loss:\t\t0.159800\n",
      "  validation accuracy:\t\t91.00 %\n",
      "Epoch 86 of 100 took 0.135s\n",
      "  training loss:\t\t0.113009\n",
      "  validation loss:\t\t0.159175\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 87 of 100 took 0.135s\n",
      "  training loss:\t\t0.111385\n",
      "  validation loss:\t\t0.160019\n",
      "  validation accuracy:\t\t90.67 %\n",
      "Epoch 88 of 100 took 0.135s\n",
      "  training loss:\t\t0.109888\n",
      "  validation loss:\t\t0.159474\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 89 of 100 took 0.135s\n",
      "  training loss:\t\t0.108428\n",
      "  validation loss:\t\t0.159616\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 90 of 100 took 0.138s\n",
      "  training loss:\t\t0.107016\n",
      "  validation loss:\t\t0.160382\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 91 of 100 took 0.138s\n",
      "  training loss:\t\t0.105811\n",
      "  validation loss:\t\t0.159846\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 92 of 100 took 0.136s\n",
      "  training loss:\t\t0.104240\n",
      "  validation loss:\t\t0.159146\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 93 of 100 took 0.135s\n",
      "  training loss:\t\t0.102953\n",
      "  validation loss:\t\t0.159949\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 94 of 100 took 0.135s\n",
      "  training loss:\t\t0.101556\n",
      "  validation loss:\t\t0.159538\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 95 of 100 took 0.141s\n",
      "  training loss:\t\t0.099990\n",
      "  validation loss:\t\t0.159881\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 96 of 100 took 0.140s\n",
      "  training loss:\t\t0.098861\n",
      "  validation loss:\t\t0.160343\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 97 of 100 took 0.138s\n",
      "  training loss:\t\t0.097640\n",
      "  validation loss:\t\t0.159415\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 98 of 100 took 0.145s\n",
      "  training loss:\t\t0.096354\n",
      "  validation loss:\t\t0.159977\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 99 of 100 took 0.144s\n",
      "  training loss:\t\t0.094958\n",
      "  validation loss:\t\t0.160223\n",
      "  validation accuracy:\t\t90.83 %\n",
      "Epoch 100 of 100 took 0.142s\n",
      "  training loss:\t\t0.094039\n",
      "  validation loss:\t\t0.160342\n",
      "  validation accuracy:\t\t90.83 %\n"
     ]
    }
   ],
   "source": [
    "# We iterate over epochs:\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, 100, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 50, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "    \n",
    "    perf.loc[epoch] = [train_err / train_batches, val_err / train_batches, val_acc / val_batches]\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / train_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Training of the net. \n",
    "\n",
    "After the definition of the network the network has to be trained. Therefore the data is split automatically into 80% training set and 20% test set (controlled by `eval_size=0.2`). We use the first 3000 samples as the training set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting procedure / log loss\n",
    "The log-loss $J(\\theta)$ is calculated for the training-set and validation set. It is defined as follows:\n",
    "<img src=\"imgs/logloss.png\" width=\"700\"/>\n",
    "\n",
    "\n",
    "\n",
    "It is plotted for each epoch below. An  epoch is defined by touching each member of the training set once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes.AxesSubplot at 0x117cca990>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEKCAYAAADdBdT9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nURSlVvx/ndcnioR7uv0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117cf5750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "perf[['train_loss','valid_loss','valid_accuracy']].plot(title='Performance during Training', ylim=(0,0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (evaluate the learning phase)\n",
    "* What is the value of the log-loss for perfect classification?\n",
    "* [optional] What is the value for the log-loss for random assignment to the classes?\n",
    "* Describe the form of the curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (LSG)\n",
    "* 0 since log(1)=0\n",
    "* For random assinment the probability for each class ist 1/10. So ln(1/10) = -2.3. Hence J = 2.3\n",
    "* The training loss reaches the optimal value of 0 but the validation loss on the untouched validation set goes to goes up a again. This indicates overfitting to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (evaluate on testset)\n",
    "Now that we have fixed the weights of the network, we can make predictions on unseen data 3000-4000 and evaluate the performance. We first have to compile a function using the input and output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91700000000000004"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = theano.function([input_var],[test_prediction])\n",
    "res = pred(X[3000:4000])[0]\n",
    "np.mean(np.argmax(res, axis=1) == y[3000:4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the notebook [MinimalLasagneCNN.ipynb](MinimalLasagneCNN.ipynb) we show how to use a CNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
